import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import re
from flask import Flask, jsonify, request
from flask_cors import CORS
import time
import logging
from urllib.parse import urljoin, urlparse
import hashlib
import random
from collections import defaultdict

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Updated news sources with working URLs
NEWS_SOURCES = {
    "hindi": [
        {"name": "Aaj Tak", "url": "https://www.aajtak.in"},
        {"name": "ABP News", "url": "https://www.abplive.com"},
        {"name": "News18 India", "url": "https://hindi.news18.com"},
        {"name": "Zee News", "url": "https://zeenews.india.com"},
        {"name": "India Today", "url": "https://www.indiatoday.in/india"}
    ],
    "kannada": [
        {"name": "TV9 Kannada", "url": "https://tv9kannada.com"},
        {"name": "Public TV", "url": "https://publictv.in"},
        {"name": "NewsFirst Kannada", "url": "https://newsfirstlive.com"},
        {"name": "Kannada Prabha", "url": "https://www.kannadaprabha.com"},
        {"name": "Udayavani", "url": "https://www.udayavani.com"}
    ],
    "bengali": [
        {"name": "News18 Bangla", "url": "https://bengali.news18.com"},
        {"name": "TV9 Bangla", "url": "https://www.tv9bangla.com"},
        {"name": "Zee 24 Ghanta", "url": "https://zeenews.india.com/bengali"},
        {"name": "Anandabazar", "url": "https://www.anandabazar.com"},
        {"name": "Ei Samay", "url": "https://eisamay.indiatimes.com"}
    ]
}

# Comprehensive ministry keywords with proper weighting system
MINISTRY_KEYWORDS = {
    "health": {
        "high_priority": {
            # English
            "health", "hospital", "doctor", "medicine", "medical", "healthcare", "treatment", "disease", "illness", "patient", "clinic", "pharmacy", "vaccination", "vaccine", "covid", "corona", "pandemic", "epidemic",
            # Hindi
            "‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø", "‡§Ö‡§∏‡•ç‡§™‡§§‡§æ‡§≤", "‡§°‡•â‡§ï‡•ç‡§ü‡§∞", "‡§¶‡§µ‡§æ", "‡§á‡§≤‡§æ‡§ú", "‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ", "‡§∞‡•ã‡§ó", "‡§¨‡•Ä‡§Æ‡§æ‡§∞‡•Ä", "‡§Æ‡§∞‡•Ä‡§ú", "‡§¶‡§µ‡§æ‡§ñ‡§æ‡§®‡§æ", "‡§ü‡•Ä‡§ï‡§æ‡§ï‡§∞‡§£", "‡§µ‡•à‡§ï‡•ç‡§∏‡•Ä‡§®", "‡§ï‡•ã‡§µ‡§ø‡§°", "‡§Æ‡§π‡§æ‡§Æ‡§æ‡§∞‡•Ä",
            # Bengali
            "‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø", "‡¶π‡¶æ‡¶∏‡¶™‡¶æ‡¶§‡¶æ‡¶≤", "‡¶°‡¶æ‡¶ï‡ßç‡¶§‡¶æ‡¶∞", "‡¶ì‡¶∑‡ßÅ‡¶ß", "‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ", "‡¶∞‡ßã‡¶ó", "‡¶Ö‡¶∏‡ßÅ‡¶ñ", "‡¶∞‡ßã‡¶ó‡ßÄ", "‡¶ï‡ßç‡¶≤‡¶ø‡¶®‡¶ø‡¶ï", "‡¶ü‡¶ø‡¶ï‡¶æ", "‡¶ï‡ßã‡¶≠‡¶ø‡¶°", "‡¶Æ‡¶π‡¶æ‡¶Æ‡¶æ‡¶∞‡ßÄ",
            # Kannada
            "‡≤Ü‡≤∞‡≥ã‡≤ó‡≥ç‡≤Ø", "‡≤Ü‡≤∏‡≥ç‡≤™‡≤§‡≥ç‡≤∞‡≥Ü", "‡≤µ‡≥à‡≤¶‡≥ç‡≤Ø", "‡≤î‡≤∑‡≤ß", "‡≤ö‡≤ø‡≤ï‡≤ø‡≤§‡≥ç‡≤∏‡≥Ü", "‡≤∞‡≥ã‡≤ó", "‡≤Ö‡≤®‡≤æ‡≤∞‡≥ã‡≤ó‡≥ç‡≤Ø", "‡≤∞‡≥ã‡≤ó‡≤ø", "‡≤ï‡≥ç‡≤≤‡≤ø‡≤®‡≤ø‡≤ï‡≥ç", "‡≤≤‡≤∏‡≤ø‡≤ï‡≥Ü", "‡≤ï‡≥ã‡≤µ‡≤ø‡≤°‡≥ç"
        },
        "medium_priority": {
            "nutrition", "diet", "food safety", "mental health", "surgery", "cancer", "diabetes", "heart", "blood pressure",
            "‡§™‡•ã‡§∑‡§£", "‡§Ü‡§π‡§æ‡§∞", "‡§ñ‡§æ‡§¶‡•ç‡§Ø ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ", "‡§Æ‡§æ‡§®‡§∏‡§ø‡§ï ‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø", "‡§∂‡§≤‡•ç‡§Ø‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ", "‡§ï‡•à‡§Ç‡§∏‡§∞", "‡§Æ‡§ß‡•Å‡§Æ‡•á‡§π", "‡§π‡•É‡§¶‡§Ø", "‡§∞‡§ï‡•ç‡§§‡§ö‡§æ‡§™",
            "‡¶™‡ßÅ‡¶∑‡ßç‡¶ü‡¶ø", "‡¶ñ‡¶æ‡¶¶‡ßç‡¶Ø", "‡¶ñ‡¶æ‡¶¶‡ßç‡¶Ø ‡¶®‡¶ø‡¶∞‡¶æ‡¶™‡¶§‡ßç‡¶§‡¶æ", "‡¶Æ‡¶æ‡¶®‡¶∏‡¶ø‡¶ï ‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø", "‡¶∂‡¶≤‡ßç‡¶Ø‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ", "‡¶ï‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶∏‡¶æ‡¶∞", "‡¶°‡¶æ‡¶Ø‡¶º‡¶æ‡¶¨‡ßá‡¶ü‡¶ø‡¶∏", "‡¶π‡ßÉ‡¶¶‡¶Ø‡¶º", "‡¶∞‡¶ï‡ßç‡¶§‡¶ö‡¶æ‡¶™",
            "‡≤™‡≥ã‡≤∑‡≤£‡≥Ü", "‡≤Ü‡≤π‡≤æ‡≤∞", "‡≤Æ‡≤æ‡≤®‡≤∏‡≤ø‡≤ï ‡≤Ü‡≤∞‡≥ã‡≤ó‡≥ç‡≤Ø", "‡≤∂‡≤∏‡≥ç‡≤§‡≥ç‡≤∞‡≤ö‡≤ø‡≤ï‡≤ø‡≤§‡≥ç‡≤∏‡≥Ü", "‡≤ï‡≥ç‡≤Ø‡≤æ‡≤®‡≥ç‡≤∏‡≤∞‡≥ç", "‡≤Æ‡≤ß‡≥Å‡≤Æ‡≥á‡≤π", "‡≤π‡≥É‡≤¶‡≤Ø"
        },
        "low_priority": {
            "fitness", "exercise", "vitamin", "protein", "wellness", "hygiene",
            "‡§´‡§ø‡§ü‡§®‡•á‡§∏", "‡§µ‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§Æ", "‡§µ‡§ø‡§ü‡§æ‡§Æ‡§ø‡§®", "‡§™‡•ç‡§∞‡•ã‡§ü‡•Ä‡§®", "‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ‡§§‡§æ",
            "‡¶´‡¶ø‡¶ü‡¶®‡ßá‡¶∏", "‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º‡¶æ‡¶Æ", "‡¶≠‡¶ø‡¶ü‡¶æ‡¶Æ‡¶ø‡¶®", "‡¶™‡ßç‡¶∞‡ßã‡¶ü‡¶ø‡¶®", "‡¶∏‡ßç‡¶¨‡¶ö‡ßç‡¶õ‡¶§‡¶æ",
            "‡≤´‡≤ø‡≤ü‡≥ç‡≤®‡≥Ü‡≤∏‡≥ç", "‡≤µ‡≥ç‡≤Ø‡≤æ‡≤Ø‡≤æ‡≤Æ", "‡≤µ‡≤ø‡≤ü‡≤Æ‡≤ø‡≤®‡≥ç", "‡≤™‡≥ç‡≤∞‡≥ã‡≤ü‡≥Ä‡≤®‡≥ç"
        }
    },
    
    "finance": {
        "high_priority": {
            "finance", "economy", "budget", "tax", "gst", "bank", "banking", "money", "currency", "rupee", "investment", "stock", "market", "inflation", "gdp",
            "‡§µ‡§ø‡§§‡•ç‡§§", "‡§Ö‡§∞‡•ç‡§•‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ", "‡§¨‡§ú‡§ü", "‡§ï‡§∞", "‡§ú‡•Ä‡§è‡§∏‡§ü‡•Ä", "‡§¨‡•à‡§Ç‡§ï", "‡§¨‡•à‡§Ç‡§ï‡§ø‡§Ç‡§ó", "‡§™‡•à‡§∏‡§æ", "‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ", "‡§∞‡•Å‡§™‡§Ø‡§æ", "‡§®‡§ø‡§µ‡•á‡§∂", "‡§∂‡•á‡§Ø‡§∞", "‡§¨‡§æ‡§ú‡§æ‡§∞", "‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ‡§∏‡•ç‡§´‡•Ä‡§§‡§ø",
            "‡¶Ö‡¶∞‡ßç‡¶•", "‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø", "‡¶¨‡¶æ‡¶ú‡ßá‡¶ü", "‡¶ï‡¶∞", "‡¶ú‡¶ø‡¶è‡¶∏‡¶ü‡¶ø", "‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï", "‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï‡¶ø‡¶Ç", "‡¶ü‡¶æ‡¶ï‡¶æ", "‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ", "‡¶¨‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡ßã‡¶ó", "‡¶∂‡ßá‡¶Ø‡¶º‡¶æ‡¶∞", "‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞", "‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø",
            "‡≤π‡≤£‡≤ï‡≤æ‡≤∏‡≥Å", "‡≤Ü‡≤∞‡≥ç‡≤•‡≤ø‡≤ï‡≤§‡≥Ü", "‡≤¨‡≤ú‡≥Ü‡≤ü‡≥ç", "‡≤§‡≥Ü‡≤∞‡≤ø‡≤ó‡≥Ü", "‡≤¨‡≥ç‡≤Ø‡≤æ‡≤Ç‡≤ï‡≥ç", "‡≤π‡≤£", "‡≤π‡≥Ç‡≤°‡≤ø‡≤ï‡≥Ü", "‡≤Æ‡≤æ‡≤∞‡≥Å‡≤ï‡≤ü‡≥ç‡≤ü‡≥Ü"
        },
        "medium_priority": {
            "business", "trade", "export", "import", "loan", "credit", "debt", "revenue", "profit", "loss",
            "‡§µ‡•ç‡§Ø‡§æ‡§™‡§æ‡§∞", "‡§µ‡•ç‡§Ø‡§µ‡§∏‡§æ‡§Ø", "‡§®‡§ø‡§∞‡•ç‡§Ø‡§æ‡§§", "‡§Ü‡§Ø‡§æ‡§§", "‡§ã‡§£", "‡§ï‡•ç‡§∞‡•á‡§°‡§ø‡§ü", "‡§Ü‡§Ø", "‡§≤‡§æ‡§≠", "‡§π‡§æ‡§®‡§ø",
            "‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡¶æ", "‡¶¨‡¶æ‡¶£‡¶ø‡¶ú‡ßç‡¶Ø", "‡¶∞‡¶™‡ßç‡¶§‡¶æ‡¶®‡¶ø", "‡¶Ü‡¶Æ‡¶¶‡¶æ‡¶®‡¶ø", "‡¶ã‡¶£", "‡¶Ü‡¶Ø‡¶º", "‡¶≤‡¶æ‡¶≠", "‡¶ï‡ßç‡¶∑‡¶§‡¶ø",
            "‡≤µ‡≥ç‡≤Ø‡≤æ‡≤™‡≤æ‡≤∞", "‡≤∞‡≤´‡≥ç‡≤§‡≥Å", "‡≤Ü‡≤Æ‡≤¶‡≥Å", "‡≤∏‡≤æ‡≤≤", "‡≤Ü‡≤¶‡≤æ‡≤Ø", "‡≤≤‡≤æ‡≤≠"
        },
        "low_priority": {
            "financial", "economic", "fiscal", "monetary", "corporate", "company",
            "‡§µ‡§ø‡§§‡•ç‡§§‡•Ä‡§Ø", "‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï", "‡§∞‡§æ‡§ú‡§ï‡•ã‡§∑‡•Ä‡§Ø", "‡§Æ‡•å‡§¶‡•ç‡§∞‡§ø‡§ï", "‡§ï‡§Ç‡§™‡§®‡•Ä",
            "‡¶Ü‡¶∞‡ßç‡¶•‡¶ø‡¶ï", "‡¶Ü‡¶∞‡ßç‡¶•‡¶ø‡¶ï", "‡¶ï‡ßã‡¶Æ‡ßç‡¶™‡¶æ‡¶®‡¶ø",
            "‡≤Ü‡≤∞‡≥ç‡≤•‡≤ø‡≤ï", "‡≤ï‡≤Ç‡≤™‡≤®‡≤ø"
        }
    },
    
    "education": {
        "high_priority": {
            "education", "school", "college", "university", "student", "teacher", "exam", "admission", "degree", "scholarship", "learning",
            "‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ", "‡§∏‡•ç‡§ï‡•Ç‡§≤", "‡§ï‡•â‡§≤‡•á‡§ú", "‡§µ‡§ø‡§∂‡•ç‡§µ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§≤‡§Ø", "‡§õ‡§æ‡§§‡•ç‡§∞", "‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï", "‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§æ", "‡§™‡•ç‡§∞‡§µ‡•á‡§∂", "‡§°‡§ø‡§ó‡•ç‡§∞‡•Ä", "‡§õ‡§æ‡§§‡•ç‡§∞‡§µ‡•É‡§§‡•ç‡§§‡§ø", "‡§Ö‡§ß‡•ç‡§Ø‡§Ø‡§®",
            "‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ", "‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤", "‡¶ï‡¶≤‡ßá‡¶ú", "‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶¨‡¶ø‡¶¶‡ßç‡¶Ø‡¶æ‡¶≤‡¶Ø‡¶º", "‡¶õ‡¶æ‡¶§‡ßç‡¶∞", "‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶ï", "‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ", "‡¶≠‡¶∞‡ßç‡¶§‡¶ø", "‡¶°‡¶ø‡¶ó‡ßç‡¶∞‡¶ø", "‡¶¨‡ßÉ‡¶§‡ßç‡¶§‡¶ø", "‡¶Ö‡¶ß‡ßç‡¶Ø‡¶Ø‡¶º‡¶®",
            "‡≤∂‡≤ø‡≤ï‡≥ç‡≤∑‡≤£", "‡≤∂‡≤æ‡≤≤‡≥Ü", "‡≤ï‡≤æ‡≤≤‡≥á‡≤ú‡≥Å", "‡≤µ‡≤ø‡≤∂‡≥ç‡≤µ‡≤µ‡≤ø‡≤¶‡≥ç‡≤Ø‡≤æ‡≤≤‡≤Ø", "‡≤µ‡≤ø‡≤¶‡≥ç‡≤Ø‡≤æ‡≤∞‡≥ç‡≤•‡≤ø", "‡≤∂‡≤ø‡≤ï‡≥ç‡≤∑‡≤ï", "‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü", "‡≤™‡≥ç‡≤∞‡≤µ‡≥á‡≤∂", "‡≤™‡≤¶‡≤µ‡≤ø", "‡≤µ‡≤ø‡≤¶‡≥ç‡≤Ø‡≤æ‡≤∞‡≥ç‡≤•‡≤ø‡≤µ‡≥á‡≤§‡≤®"
        },
        "medium_priority": {
            "classroom", "curriculum", "academic", "grade", "result", "mark", "score", "literacy", "research",
            "‡§ï‡§ï‡•ç‡§∑‡§æ", "‡§™‡§æ‡§†‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ", "‡§∂‡•à‡§ï‡•ç‡§∑‡§£‡§ø‡§ï", "‡§ó‡•ç‡§∞‡•á‡§°", "‡§™‡§∞‡§ø‡§£‡§æ‡§Æ", "‡§Ö‡§Ç‡§ï", "‡§∏‡§æ‡§ï‡•ç‡§∑‡§∞‡§§‡§æ", "‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§®",
            "‡¶∂‡ßç‡¶∞‡ßá‡¶£‡ßÄ‡¶ï‡¶ï‡ßç‡¶∑", "‡¶™‡¶æ‡¶†‡ßç‡¶Ø‡¶ï‡ßç‡¶∞‡¶Æ", "‡¶è‡¶ï‡¶æ‡¶°‡ßá‡¶Æ‡¶ø‡¶ï", "‡¶ó‡ßç‡¶∞‡ßá‡¶°", "‡¶´‡¶≤‡¶æ‡¶´‡¶≤", "‡¶®‡¶Æ‡ßç‡¶¨‡¶∞", "‡¶∏‡¶æ‡¶ï‡ßç‡¶∑‡¶∞‡¶§‡¶æ", "‡¶ó‡¶¨‡ßá‡¶∑‡¶£‡¶æ",
            "‡≤§‡≤∞‡≤ó‡≤§‡≤ø", "‡≤™‡≤†‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∞‡≤Æ", "‡≤∂‡≥à‡≤ï‡≥ç‡≤∑‡≤£‡≤ø‡≤ï", "‡≤´‡≤≤‡≤ø‡≤§‡≤æ‡≤Ç‡≤∂", "‡≤∏‡≤Ç‡≤∂‡≥ã‡≤ß‡≤®‡≥Ü"
        },
        "low_priority": {
            "knowledge", "skill", "training", "workshop", "seminar",
            "‡§ú‡•ç‡§û‡§æ‡§®", "‡§ï‡•å‡§∂‡§≤", "‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£", "‡§ï‡§æ‡§∞‡•ç‡§Ø‡§∂‡§æ‡§≤‡§æ", "‡§∏‡§Ç‡§ó‡•ã‡§∑‡•ç‡§†‡•Ä",
            "‡¶ú‡ßç‡¶û‡¶æ‡¶®", "‡¶¶‡¶ï‡ßç‡¶∑‡¶§‡¶æ", "‡¶™‡ßç‡¶∞‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶£", "‡¶ï‡¶∞‡ßç‡¶Æ‡¶∂‡¶æ‡¶≤‡¶æ", "‡¶∏‡ßá‡¶Æ‡¶ø‡¶®‡¶æ‡¶∞",
            "‡≤ú‡≥ç‡≤û‡≤æ‡≤®", "‡≤ï‡≥å‡≤∂‡≤≤‡≥ç‡≤Ø", "‡≤§‡≤∞‡≤¨‡≥á‡≤§‡≤ø"
        }
    },
    
    "sports": {
        "high_priority": {
            "sports", "cricket", "football", "hockey", "tennis", "badminton", "olympics", "player", "match", "tournament", "game", "championship",
            "‡§ñ‡•á‡§≤", "‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü", "‡§´‡•Å‡§ü‡§¨‡•â‡§≤", "‡§π‡•â‡§ï‡•Ä", "‡§ü‡•á‡§®‡§ø‡§∏", "‡§¨‡•à‡§°‡§Æ‡§ø‡§Ç‡§ü‡§®", "‡§ì‡§≤‡§Ç‡§™‡§ø‡§ï", "‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä", "‡§Æ‡•à‡§ö", "‡§ü‡•Ç‡§∞‡•ç‡§®‡§æ‡§Æ‡•á‡§Ç‡§ü", "‡§ñ‡•á‡§≤", "‡§ö‡•à‡§Ç‡§™‡§ø‡§Ø‡§®‡§∂‡§ø‡§™",
            "‡¶ñ‡ßá‡¶≤‡¶æ", "‡¶ï‡ßç‡¶∞‡¶ø‡¶ï‡ßá‡¶ü", "‡¶´‡ßÅ‡¶ü‡¶¨‡¶≤", "‡¶π‡¶ï‡¶ø", "‡¶ü‡ßá‡¶®‡¶ø‡¶∏", "‡¶¨‡ßç‡¶Ø‡¶æ‡¶°‡¶Æ‡¶ø‡¶®‡ßç‡¶ü‡¶®", "‡¶Ö‡¶≤‡¶ø‡¶Æ‡ßç‡¶™‡¶ø‡¶ï", "‡¶ñ‡ßá‡¶≤‡ßã‡¶Ø‡¶º‡¶æ‡¶°‡¶º", "‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ö", "‡¶ü‡ßÅ‡¶∞‡ßç‡¶®‡¶æ‡¶Æ‡ßá‡¶®‡ßç‡¶ü", "‡¶ö‡ßç‡¶Ø‡¶æ‡¶Æ‡ßç‡¶™‡¶ø‡¶Ø‡¶º‡¶®‡¶∂‡¶ø‡¶™",
            "‡≤ï‡≥ç‡≤∞‡≥Ä‡≤°‡≥Ü", "‡≤ï‡≥ç‡≤∞‡≤ø‡≤ï‡≥Ü‡≤ü‡≥ç", "‡≤´‡≥Å‡≤ü‡≥ç‡≤¨‡≤æ‡≤≤‡≥ç", "‡≤π‡≤æ‡≤ï‡≤ø", "‡≤ü‡≥Ü‡≤®‡≥ç‡≤®‡≤ø‡≤∏‡≥ç", "‡≤¨‡≥ç‡≤Ø‡≤æ‡≤°‡≥ç‡≤Æ‡≤ø‡≤Ç‡≤ü‡≤®‡≥ç", "‡≤í‡≤≤‡≤ø‡≤Ç‡≤™‡≤ø‡≤ï‡≥ç‡≤∏‡≥ç", "‡≤Ü‡≤ü‡≤ó‡≤æ‡≤∞", "‡≤™‡≤Ç‡≤¶‡≥ç‡≤Ø", "‡≤ü‡≥Ç‡≤∞‡≥ç‡≤®‡≤Æ‡≥Ü‡≤Ç‡≤ü‡≥ç"
        },
        "medium_priority": {
            "athlete", "coach", "team", "medal", "winner", "champion", "score", "goal", "run", "wicket",
            "‡§è‡§•‡§≤‡•Ä‡§ü", "‡§ï‡•ã‡§ö", "‡§ü‡•Ä‡§Æ", "‡§™‡§¶‡§ï", "‡§µ‡§ø‡§ú‡•á‡§§‡§æ", "‡§ö‡•à‡§Ç‡§™‡§ø‡§Ø‡§®", "‡§∏‡•ç‡§ï‡•ã‡§∞", "‡§ó‡•ã‡§≤", "‡§∞‡§®", "‡§µ‡§ø‡§ï‡•á‡§ü",
            "‡¶Ö‡ßç‡¶Ø‡¶æ‡¶•‡¶≤‡¶ø‡¶ü", "‡¶ï‡ßã‡¶ö", "‡¶¶‡¶≤", "‡¶™‡¶¶‡¶ï", "‡¶¨‡¶ø‡¶ú‡¶Ø‡¶º‡ßÄ", "‡¶ö‡ßç‡¶Ø‡¶æ‡¶Æ‡ßç‡¶™‡¶ø‡¶Ø‡¶º‡¶®", "‡¶∏‡ßç‡¶ï‡ßã‡¶∞", "‡¶ó‡ßã‡¶≤", "‡¶∞‡¶æ‡¶®", "‡¶â‡¶á‡¶ï‡ßá‡¶ü",
            "‡≤Ö‡≤•‡≥ç‡≤≤‡≥Ä‡≤ü‡≥ç", "‡≤§‡≤∞‡≤¨‡≥á‡≤§‡≥Å‡≤¶‡≤æ‡≤∞", "‡≤§‡≤Ç‡≤°", "‡≤™‡≤¶‡≤ï", "‡≤µ‡≤ø‡≤ú‡≥á‡≤§", "‡≤ö‡≤æ‡≤Ç‡≤™‡≤ø‡≤Ø‡≤®‡≥ç", "‡≤ó‡≥ã‡≤≤‡≥ç", "‡≤∞‡≤®‡≥ç"
        },
        "low_priority": {
            "stadium", "ground", "field", "fitness", "training", "practice",
            "‡§∏‡•ç‡§ü‡•á‡§°‡§ø‡§Ø‡§Æ", "‡§Æ‡•à‡§¶‡§æ‡§®", "‡§´‡§ø‡§ü‡§®‡•á‡§∏", "‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£", "‡§Ö‡§≠‡•ç‡§Ø‡§æ‡§∏",
            "‡¶∏‡ßç‡¶ü‡ßá‡¶°‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ", "‡¶Æ‡¶æ‡¶†", "‡¶´‡¶ø‡¶ü‡¶®‡ßá‡¶∏", "‡¶™‡ßç‡¶∞‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶£", "‡¶Ö‡¶®‡ßÅ‡¶∂‡ßÄ‡¶≤‡¶®",
            "‡≤ï‡≥ç‡≤∞‡≥Ä‡≤°‡≤æ‡≤Ç‡≤ó‡≤£", "‡≤Æ‡≥à‡≤¶‡≤æ‡≤®", "‡≤´‡≤ø‡≤ü‡≥ç‡≤®‡≥Ü‡≤∏‡≥ç", "‡≤§‡≤∞‡≤¨‡≥á‡≤§‡≤ø"
        }
    },
    
    "international_affairs": {
        "high_priority": {
            "international", "foreign", "diplomat", "embassy", "visa", "border", "treaty", "agreement", "summit", "bilateral", "multilateral",
            "‡§Ö‡§Ç‡§§‡§∞‡•ç‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Ø", "‡§µ‡§ø‡§¶‡•á‡§∂‡•Ä", "‡§∞‡§æ‡§ú‡§¶‡•Ç‡§§", "‡§¶‡•Ç‡§§‡§æ‡§µ‡§æ‡§∏", "‡§µ‡•Ä‡§ú‡§æ", "‡§∏‡•Ä‡§Æ‡§æ", "‡§∏‡§Ç‡§ß‡§ø", "‡§∏‡§Æ‡§ù‡•å‡§§‡§æ", "‡§∂‡§ø‡§ñ‡§∞ ‡§∏‡§Æ‡•ç‡§Æ‡•á‡§≤‡§®", "‡§¶‡•ç‡§µ‡§ø‡§™‡§ï‡•ç‡§∑‡•Ä‡§Ø",
            "‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï", "‡¶¨‡¶ø‡¶¶‡ßá‡¶∂‡¶ø", "‡¶∞‡¶æ‡¶∑‡ßç‡¶ü‡ßç‡¶∞‡¶¶‡ßÇ‡¶§", "‡¶¶‡ßÇ‡¶§‡¶æ‡¶¨‡¶æ‡¶∏", "‡¶≠‡¶ø‡¶∏‡¶æ", "‡¶∏‡ßÄ‡¶Æ‡¶æ‡¶®‡¶æ", "‡¶ö‡ßÅ‡¶ï‡ßç‡¶§‡¶ø", "‡¶∏‡¶Æ‡¶ù‡ßã‡¶§‡¶æ", "‡¶∂‡ßÄ‡¶∞‡ßç‡¶∑ ‡¶∏‡¶Æ‡ßç‡¶Æ‡ßá‡¶≤‡¶®", "‡¶¶‡ßç‡¶¨‡¶ø‡¶™‡¶æ‡¶ï‡ßç‡¶∑‡¶ø‡¶ï",
            "‡≤Ö‡≤Ç‡≤§‡≤∞‡≤∞‡≤æ‡≤∑‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Ø", "‡≤µ‡≤ø‡≤¶‡≥á‡≤∂‡≤ø", "‡≤∞‡≤æ‡≤Ø‡≤≠‡≤æ‡≤∞‡≤ø", "‡≤¶‡≥Ç‡≤§‡≤æ‡≤µ‡≤æ‡≤∏", "‡≤µ‡≥Ä‡≤∏‡≤æ", "‡≤ó‡≤°‡≤ø", "‡≤í‡≤™‡≥ç‡≤™‡≤Ç‡≤¶", "‡≤∂‡≥É‡≤Ç‡≤ó‡≤∏‡≤≠‡≥Ü"
        },
        "medium_priority": {
            "china", "pakistan", "america", "usa", "russia", "bangladesh", "nepal", "sri lanka", "trade war", "sanction",
            "‡§ö‡•Ä‡§®", "‡§™‡§æ‡§ï‡§ø‡§∏‡•ç‡§§‡§æ‡§®", "‡§Ö‡§Æ‡•á‡§∞‡§ø‡§ï‡§æ", "‡§∞‡•Ç‡§∏", "‡§¨‡§æ‡§Ç‡§ó‡•ç‡§≤‡§æ‡§¶‡•á‡§∂", "‡§®‡•á‡§™‡§æ‡§≤", "‡§∂‡•ç‡§∞‡•Ä‡§≤‡§Ç‡§ï‡§æ", "‡§µ‡•ç‡§Ø‡§æ‡§™‡§æ‡§∞ ‡§Ø‡•Å‡§¶‡•ç‡§ß", "‡§™‡•ç‡§∞‡§§‡§ø‡§¨‡§Ç‡§ß",
            "‡¶ö‡ßÄ‡¶®", "‡¶™‡¶æ‡¶ï‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶®", "‡¶Ü‡¶Æ‡ßá‡¶∞‡¶ø‡¶ï‡¶æ", "‡¶∞‡¶æ‡¶∂‡¶ø‡¶Ø‡¶º‡¶æ", "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂", "‡¶®‡ßá‡¶™‡¶æ‡¶≤", "‡¶∂‡ßç‡¶∞‡ßÄ‡¶≤‡¶ô‡ßç‡¶ï‡¶æ", "‡¶¨‡¶æ‡¶£‡¶ø‡¶ú‡ßç‡¶Ø ‡¶Ø‡ßÅ‡¶¶‡ßç‡¶ß", "‡¶®‡¶ø‡¶∑‡ßá‡¶ß‡¶æ‡¶ú‡ßç‡¶û‡¶æ",
            "‡≤ö‡≥Ä‡≤®‡≤æ", "‡≤™‡≤æ‡≤ï‡≤ø‡≤∏‡≥ç‡≤§‡≤æ‡≤®", "‡≤Ö‡≤Æ‡≥á‡≤∞‡≤ø‡≤ï‡≤æ", "‡≤∞‡≤∑‡≥ç‡≤Ø‡≤æ", "‡≤¨‡≤æ‡≤Ç‡≤ó‡≥ç‡≤≤‡≤æ‡≤¶‡≥á‡≤∂", "‡≤®‡≥á‡≤™‡≤æ‡≤≥", "‡≤∂‡≥ç‡≤∞‡≥Ä‡≤≤‡≤Ç‡≤ï‡≤æ"
        },
        "low_priority": {
            "global", "world", "united nations", "un", "nato", "g7", "g20",
            "‡§µ‡•à‡§∂‡•ç‡§µ‡§ø‡§ï", "‡§µ‡§ø‡§∂‡•ç‡§µ", "‡§∏‡§Ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞", "‡§ú‡•Ä20",
            "‡¶¨‡ßà‡¶∂‡ßç‡¶¨‡¶ø‡¶ï", "‡¶¨‡¶ø‡¶∂‡ßç‡§µ", "‡¶ú‡¶æ‡¶§‡¶ø‡¶∏‡¶Ç‡¶ò", "‡¶ú‡¶ø‡ß®‡ß¶",
            "‡≤ú‡≤æ‡≤ó‡≤§‡≤ø‡≤ï", "‡≤™‡≥ç‡≤∞‡≤™‡≤Ç‡≤ö", "‡≤µ‡≤ø‡≤∂‡≥ç‡≤µ‡≤∏‡≤Ç‡≤∏‡≥ç‡≤•‡≥Ü"
        }
    },
    
    "agriculture": {
        "high_priority": {
            "agriculture", "farming", "farmer", "crop", "harvest", "irrigation", "fertilizer", "seed", "pesticide", "soil",
            "‡§ï‡•É‡§∑‡§ø", "‡§ñ‡•á‡§§‡•Ä", "‡§ï‡§ø‡§∏‡§æ‡§®", "‡§´‡§∏‡§≤", "‡§ï‡§ü‡§æ‡§à", "‡§∏‡§ø‡§Ç‡§ö‡§æ‡§à", "‡§â‡§∞‡•ç‡§µ‡§∞‡§ï", "‡§¨‡•Ä‡§ú", "‡§ï‡•Ä‡§ü‡§®‡§æ‡§∂‡§ï", "‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä",
            "‡¶ï‡ßÉ‡¶∑‡¶ø", "‡¶ö‡¶æ‡¶∑‡¶æ‡¶¨‡¶æ‡¶¶", "‡¶ï‡ßÉ‡¶∑‡¶ï", "‡¶´‡¶∏‡¶≤", "‡¶´‡¶∏‡¶≤ ‡¶ï‡¶æ‡¶ü‡¶æ", "‡¶∏‡ßá‡¶ö", "‡¶∏‡¶æ‡¶∞", "‡¶¨‡ßÄ‡¶ú", "‡¶ï‡ßÄ‡¶ü‡¶®‡¶æ‡¶∂‡¶ï", "‡¶Æ‡¶æ‡¶ü‡¶ø",
            "‡≤ï‡≥É‡≤∑‡≤ø", "‡≤ï‡≥É‡≤∑‡≤ø‡≤ï", "‡≤∞‡≥à‡≤§", "‡≤¨‡≥Ü‡≤≥‡≥Ü", "‡≤∏‡≥Å‡≤ó‡≥ç‡≤ó‡≤ø", "‡≤®‡≥Ä‡≤∞‡≤æ‡≤µ‡≤∞‡≤ø", "‡≤ó‡≥ä‡≤¨‡≥ç‡≤¨‡≤∞", "‡≤¨‡≥Ä‡≤ú", "‡≤Æ‡≤£‡≥ç‡≤£‡≥Å"
        },
        "medium_priority": {
            "rice", "wheat", "cotton", "sugarcane", "msp", "procurement", "subsidy", "rural", "village",
            "‡§ö‡§æ‡§µ‡§≤", "‡§ó‡•á‡§π‡•Ç‡§Ç", "‡§ï‡§™‡§æ‡§∏", "‡§ó‡§®‡•ç‡§®‡§æ", "‡§è‡§Æ‡§è‡§∏‡§™‡•Ä", "‡§ñ‡§∞‡•Ä‡§¶", "‡§∏‡§¨‡•ç‡§∏‡§ø‡§°‡•Ä", "‡§ó‡•ç‡§∞‡§æ‡§Æ‡•Ä‡§£", "‡§ó‡§æ‡§Ç‡§µ",
            "‡¶ö‡¶æ‡¶≤", "‡¶ó‡¶Æ", "‡¶§‡ßÅ‡¶≤‡¶æ", "‡¶Ü‡¶ñ", "‡¶è‡¶Æ‡¶è‡¶∏‡¶™‡¶ø", "‡¶ï‡ßç‡¶∞‡¶Ø‡¶º", "‡¶≠‡¶∞‡ßç‡¶§‡ßÅ‡¶ï‡¶ø", "‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ‡ßÄ‡¶£", "‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ",
            "‡≤Ö‡≤ï‡≥ç‡≤ï‡≤ø", "‡≤ó‡≥ã‡≤ß‡≤ø", "‡≤π‡≤§‡≥ç‡≤§‡≤ø", "‡≤ï‡≤¨‡≥ç‡≤¨‡≥Å", "‡≤é‡≤Ç‡≤é‡≤∏‡≥ç‡≤™‡≤ø", "‡≤ó‡≥ç‡≤∞‡≤æ‡≤Æ‡≥Ä‡≤£", "‡≤ó‡≥ç‡≤∞‡≤æ‡≤Æ"
        },
        "low_priority": {
            "organic", "biotechnology", "gmo", "climate change", "drought", "flood",
            "‡§ú‡•à‡§µ‡§ø‡§ï", "‡§ú‡•à‡§µ ‡§™‡•ç‡§∞‡•å‡§¶‡•ç‡§Ø‡•ã‡§ó‡§ø‡§ï‡•Ä", "‡§ú‡•Ä‡§è‡§Æ‡§ì", "‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§®", "‡§∏‡•Ç‡§ñ‡§æ", "‡§¨‡§æ‡§¢‡§º",
            "‡¶ú‡ßà‡¶¨", "‡¶ú‡ßà‡¶¨‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø", "‡¶ú‡¶ø‡¶è‡¶Æ‡¶ì", "‡¶ú‡¶≤‡¶¨‡¶æ‡¶Ø‡¶º‡ßÅ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶®", "‡¶ñ‡¶∞‡¶æ", "‡¶¨‡¶®‡ßç‡¶Ø‡¶æ",
            "‡≤∏‡≤æ‡≤µ‡≤Ø‡≤µ", "‡≤ú‡≥à‡≤µ‡≤§‡≤Ç‡≤§‡≥ç‡≤∞‡≤ú‡≥ç‡≤û‡≤æ‡≤®", "‡≤π‡≤µ‡≤æ‡≤Æ‡≤æ‡≤® ‡≤¨‡≤¶‡≤≤‡≤æ‡≤µ‡≤£‡≥Ü", "‡≤¨‡≤∞‡≤ó‡≤æ‡≤≤", "‡≤™‡≥ç‡≤∞‡≤µ‡≤æ‡≤π"
        }
    }
}

# Weight values for different priority levels
PRIORITY_WEIGHTS = {
    "high_priority": 5,
    "medium_priority": 3,
    "low_priority": 1
}

class NewsArticle:
    def __init__(self, title, content, url, source, timestamp, ministry=None, language=None, confidence=0.0):
        self.title = title
        self.content = content
        self.url = url
        self.source = source
        self.timestamp = timestamp
        self.ministry = ministry
        self.language = language
        self.confidence = confidence
        # Create unique hash for deduplication
        self.content_hash = hashlib.md5((title.lower().strip()).encode('utf-8')).hexdigest()

    def to_dict(self):
        return {
            "title": self.title,
            "content": self.content,
            "url": self.url,
            "source": self.source,
            "timestamp": self.timestamp,
            "ministry": self.ministry,
            "language": self.language,
            "confidence": round(self.confidence, 2)
        }

class NewsScraper:
    def __init__(self):
        # Rotate user agents to avoid blocking
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
        
        self.headers = {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.seen_articles = set()

    def categorize_by_ministry(self, title, content):
        """Enhanced categorization with proper weighted scoring"""
        # Combine title and content, giving title more weight
        text = f"{title} {title} {content}".lower()
        
        ministry_scores = defaultdict(float)
        
        for ministry, priority_groups in MINISTRY_KEYWORDS.items():
            total_score = 0
            keyword_matches = []
            
            for priority_level, keywords in priority_groups.items():
                weight = PRIORITY_WEIGHTS[priority_level]
                
                for keyword in keywords:
                    keyword_lower = keyword.lower()
                    # Count occurrences of the keyword
                    count = text.count(keyword_lower)
                    
                    if count > 0:
                        score = count * weight
                        total_score += score
                        keyword_matches.append(f"{keyword}({count})")
            
            if total_score > 0:
                # Normalize score based on text length to avoid bias towards longer articles
                normalized_score = total_score / max(len(text.split()), 1) * 100
                ministry_scores[ministry] = normalized_score
                
                logger.debug(f"Ministry '{ministry}': score={total_score:.2f}, normalized={normalized_score:.2f}, matches={keyword_matches[:5]}")
        
        if ministry_scores:
            # Get the best scoring ministry
            best_ministry = max(ministry_scores, key=ministry_scores.get)
            best_score = ministry_scores[best_ministry]
            
            # Set minimum confidence threshold
            MIN_CONFIDENCE = 0.5
            
            if best_score >= MIN_CONFIDENCE:
                logger.info(f"Categorized as '{best_ministry}' with confidence {best_score:.2f}")
                return best_ministry, best_score
            else:
                logger.info(f"Low confidence categorization: '{best_ministry}' ({best_score:.2f}) < threshold ({MIN_CONFIDENCE})")
        
        return "general", 0.0

    def extract_content_advanced(self, soup, base_url):
        """Improved content extraction with better text cleaning"""
        articles = []
        
        # Multiple strategies for finding articles
        selectors = [
            # Generic article selectors
            'article',
            '[class*="story"]',
            '[class*="article"]',
            '[class*="news"]',
            '[class*="post"]',
            '[class*="item"]',
            # Header selectors
            'h1, h2, h3, h4',
            # Link selectors with meaningful text
            'a[title]',
            # Content containers
            '[class*="content"] h2',
            '[class*="content"] h3',
            # News specific
            '[class*="headline"]',
            '[class*="title"]'
        ]
        
        found_articles = set()
        
        for selector in selectors:
            try:
                elements = soup.select(selector)
                
                for element in elements[:30]:  # Limit to avoid too many elements
                    try:
                        # Extract title
                        title = ""
                        
                        if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                            title = element.get_text(strip=True)
                        elif element.name == 'a' and element.get('title'):
                            title = element.get('title').strip()
                        elif element.name == 'article':
                            title_elem = element.find(['h1', 'h2', 'h3', 'h4', 'h5'])
                            if title_elem:
                                title = title_elem.get_text(strip=True)
                        else:
                            # Try to find title in the element
                            title_elem = element.find(['h1', 'h2', 'h3', 'h4', 'h5']) or element
                            title = title_elem.get_text(strip=True)
                        
                        # Clean and validate title
                        title = re.sub(r'\s+', ' ', title).strip()
                        title = re.sub(r'^[^a-zA-Z\u0900-\u097F\u0980-\u09FF\u0C80-\u0CFF]*', '', title)
                        
                        # Skip if title is too short, too long, or contains unwanted content
                        if (len(title) < 15 or len(title) > 200 or
                            any(skip in title.lower() for skip in 
                                ['advertisement', 'sponsored', 'live:', 'watch:', 'video:', 'photo:', 'gallery:', 'breaking:', 'update:']) or
                            title.lower().startswith(('http', 'www', 'click', 'see', 'view', 'more'))):
                            continue
                        
                        # Check for duplicates
                        title_hash = hashlib.md5(title.lower().encode('utf-8')).hexdigest()
                        if title_hash in found_articles:
                            continue
                        found_articles.add(title_hash)
                        
                        # Extract content/description
                        content = ""
                        parent = element.parent
                        
                        # Look for content in various ways
                        content_sources = []
                        
                        # Method 1: Look for description/summary in parent
                        if parent:
                            desc_elem = parent.find(['p', 'div', 'span'], 
                                class_=re.compile(r'(summary|excerpt|desc|intro|lead)', re.I))
                            if desc_elem:
                                content_sources.append(desc_elem.get_text(strip=True))
                        
                        # Method 2: Look for first paragraph after title
                        if element.name in ['h1', 'h2', 'h3', 'h4', 'h5']:
                            next_elem = element.find_next('p')
                            if next_elem:
                                content_sources.append(next_elem.get_text(strip=True))
                        
                        # Method 3: Look for content in article container
                        if element.name == 'article':
                            paragraphs = element.find_all('p')
                            if paragraphs:
                                content_sources.append(paragraphs[0].get_text(strip=True))
                        
                        # Method 4: Use element's own text if it's long enough
                        if len(title) < 100:  # Only for shorter titles
                            elem_text = element.get_text(strip=True)
                            if len(elem_text) > len(title) + 20:
                                remaining_text = elem_text.replace(title, '').strip()
                                if len(remaining_text) > 20:
                                    content_sources.append(remaining_text)
                        
                        # Select best content
                        for source_content in content_sources:
                            if len(source_content) >= 30 and source_content.lower() != title.lower():
                                content = source_content
                                break
                        
                        # Clean content
                        if content:
                            content = re.sub(r'\s+', ' ', content).strip()
                            # Remove title from content if it appears
                            if title.lower() in content.lower():
                                content = content.replace(title, '').strip()
                            # Truncate if too long
                            if len(content) > 400:
                                content = content[:400] + "..."
                        
                        # If no good content found, create a brief one from title
                        if not content or len(content) < 20:
                            content = f"{title[:100]}{'...' if len(title) > 100 else ''}"
                        
                        # Extract URL
                        article_url = base_url
                        link_elem = None
                        
                        if element.name == 'a':
                            link_elem = element
                        else:
                            # Look for link in or around the element
                            link_elem = (element.find('a', href=True) or 
                                       (element.parent and element.parent.find('a', href=True)) or
                                       element.find_previous('a', href=True) or
                                       element.find_next('a', href=True))
                        
                        if link_elem and link_elem.get('href'):
                            href = link_elem['href']
                            if href.startswith('http'):
                                article_url = href
                            elif href.startswith('/'):
                                article_url = urljoin(base_url, href)
                            elif href and not href.startswith('#'):
                                article_url = urljoin(base_url, href)
                        
                        articles.append({
                            'title': title,
                            'content': content,
                            'url': article_url
                        })
                        
                        # Limit to avoid too many articles per source
                        if len(articles) >= 15:
                            break
                            
                    except Exception as e:
                        logger.debug(f"Error processing element: {str(e)}")
                        continue
                
                # Break if we have enough articles
                if len(articles) >= 15:
                    break
                    
            except Exception as e:
                logger.debug(f"Error with selector {selector}: {str(e)}")
                continue
        
        # Remove duplicates and return best articles
        unique_articles = []
        seen_titles = set()
        
        for article in articles:
            title_normalized = re.sub(r'[^\w\s]', '', article['title'].lower())
            if title_normalized not in seen_titles and len(title_normalized) > 10:
                seen_titles.add(title_normalized)
                unique_articles.append(article)
        
        logger.info(f"Extracted {len(unique_articles)} unique articles from {len(articles)} total")
        return unique_articles[:10]  # Return top 10 per source

    def scrape_generic_news(self, url, source_name, language):
        """Enhanced generic scraping with better error handling and retry logic"""
        max_retries = 2
        
        for attempt in range(max_retries):
            try:
                logger.info(f"Scraping {source_name} ({language}): {url} (attempt {attempt + 1})")
                
                # Add random delay to avoid rate limiting
                time.sleep(random.uniform(1, 3))
                
                # Randomize user agent for each request
                self.headers['User-Agent'] = random.choice([
                    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
                ])
                
                response = self.session.get(url, timeout=25, allow_redirects=True)
                response.raise_for_status()
                
                # Check if we got actual HTML content
                if 'text/html' not in response.headers.get('content-type', ''):
                    logger.warning(f"Non-HTML response from {url}")
                    continue
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Remove unwanted elements
                for unwanted in soup(['script', 'style', 'nav', 'header', 'footer', 
                                    'aside', 'iframe', 'noscript', 'form', 'button']):
                    unwanted.decompose()
                
                # Extract articles using advanced method
                article_data = self.extract_content_advanced(soup, url)
                
                if not article_data:
                    logger.warning(f"No articles found for {source_name}")
                    if attempt < max_retries - 1:
                        time.sleep(5)  # Wait before retry
                        continue
                    return []
                
                articles = []
                for data in article_data:
                    try:
                        # Categorize by ministry with confidence score
                        ministry, confidence = self.categorize_by_ministry(data['title'], data['content'])
                        
                        article = NewsArticle(
                            title=data['title'],
                            content=data['content'],
                            url=data['url'],
                            source=source_name,
                            timestamp=datetime.now().isoformat(),
                            ministry=ministry,
                            language=language,
                            confidence=confidence
                        )
                        
                        articles.append(article)
                        
                    except Exception as e:
                        logger.warning(f"Error creating article: {str(e)}")
                        continue
                
                logger.info(f"Successfully scraped {len(articles)} articles from {source_name}")
                return articles
                
            except requests.exceptions.Timeout:
                logger.warning(f"Timeout scraping {url} (attempt {attempt + 1})")
                if attempt < max_retries - 1:
                    time.sleep(5)
                    continue
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 403:
                    logger.error(f"Access forbidden for {url} - skipping")
                    return []
                else:
                    logger.warning(f"HTTP error {e.response.status_code} for {url} (attempt {attempt + 1})")
                    if attempt < max_retries - 1:
                        time.sleep(5)
                        continue
            except Exception as e:
                logger.warning(f"Error scraping {url} (attempt {attempt + 1}): {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(5)
                    continue
        
        logger.error(f"Failed to scrape {url} after {max_retries} attempts")
        return []

    def scrape_news_by_language(self, language):
        """Scrape news from all sources for a specific language"""
        if language not in NEWS_SOURCES:
            logger.error(f"Language '{language}' not supported")
            return []
        
        all_articles = []
        sources = NEWS_SOURCES[language]
        
        for source in sources:
            try:
                articles = self.scrape_generic_news(source['url'], source['name'], language)
                all_articles.extend(articles)
                
                # Small delay between sources
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"Error scraping {source['name']}: {str(e)}")
                continue
        
        logger.info(f"Total articles scraped for {language}: {len(all_articles)}")
        return all_articles

    def get_news_by_ministry_and_language(self, ministry=None, language=None):
        """Get news filtered by ministry and language"""
        # Clear seen articles for fresh scraping
        self.seen_articles.clear()
        
        all_articles = []
        
        languages_to_scrape = [language] if language else list(NEWS_SOURCES.keys())
        
        for lang in languages_to_scrape:
            articles = self.scrape_news_by_language(lang)
            all_articles.extend(articles)
        
        # Filter by ministry if specified
        if ministry and ministry != 'all':
            filtered_articles = [a for a in all_articles if a.ministry == ministry]
            if filtered_articles:
                all_articles = filtered_articles
                logger.info(f"Found {len(filtered_articles)} articles for ministry: {ministry}")
            else:
                logger.warning(f"No articles found for ministry: {ministry}")
                # Return general articles as fallback
                general_articles = [a for a in all_articles if a.ministry == 'general']
                all_articles = general_articles[:5]
        
        # Sort by confidence score and timestamp
        all_articles.sort(key=lambda x: (x.confidence, x.timestamp), reverse=True)
        
        # Return appropriate number of articles
        limit = 8 if ministry else 20
        return all_articles[:limit]

# Initialize scraper
scraper = NewsScraper()

@app.route('/api/news', methods=['GET'])
def get_news():
    """Enhanced API endpoint with better filtering and statistics"""
    try:
        language = request.args.get('language', None)
        ministry = request.args.get('ministry', None)
        
        logger.info(f"API Request - Language: {language}, Ministry: {ministry}")
        
        articles = scraper.get_news_by_ministry_and_language(ministry, language)
        
        # Calculate statistics
        ministry_counts = defaultdict(int)
        language_counts = defaultdict(int)
        confidence_stats = []
        
        for article in articles:
            ministry_counts[article.ministry] += 1
            language_counts[article.language] += 1
            confidence_stats.append(article.confidence)
        
        # Calculate average confidence
        avg_confidence = sum(confidence_stats) / len(confidence_stats) if confidence_stats else 0
        
        response_data = {
            "articles": [article.to_dict() for article in articles],
            "total": len(articles),
            "statistics": {
                "ministry_distribution": dict(ministry_counts),
                "language_distribution": dict(language_counts),
                "average_confidence": round(avg_confidence, 2),
                "high_confidence_articles": len([c for c in confidence_stats if c > 2.0])
            },
            "filters": {
                "language": language,
                "ministry": ministry
            },
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"Returning {len(articles)} articles - Ministry dist: {dict(ministry_counts)} - Avg confidence: {avg_confidence:.2f}")
        return jsonify(response_data)
    
    except Exception as e:
        logger.error(f"Error in get_news endpoint: {str(e)}")
        return jsonify({
            "error": "Internal server error", 
            "message": str(e),
            "articles": [],
            "total": 0
        }), 500

@app.route('/api/languages', methods=['GET'])
def get_languages():
    """Get available languages"""
    return jsonify({
        "languages": list(NEWS_SOURCES.keys()),
        "total": len(NEWS_SOURCES)
    })

@app.route('/api/ministries', methods=['GET'])
def get_ministries():
    """Get available ministries with descriptions"""
    ministries_info = {}
    for ministry in MINISTRY_KEYWORDS.keys():
        ministries_info[ministry] = {
            "name": ministry.replace('_', ' ').title(),
            "keywords_count": sum(len(keywords) for keywords in MINISTRY_KEYWORDS[ministry].values())
        }
    
    ministries_info["general"] = {
        "name": "General",
        "keywords_count": 0
    }
    
    return jsonify({
        "ministries": ministries_info,
        "total": len(ministries_info)
    })

@app.route('/api/sources', methods=['GET'])
def get_sources():
    """Get all news sources"""
    return jsonify(NEWS_SOURCES)

@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "4.0",
        "features": {
            "improved_categorization": True,
            "confidence_scoring": True,
            "better_content_extraction": True,
            "retry_mechanism": True
        },
        "sources_count": sum(len(sources) for sources in NEWS_SOURCES.values()),
        "ministries_count": len(MINISTRY_KEYWORDS) + 1,
        "total_keywords": sum(sum(len(keywords) for keywords in ministry.values()) 
                            for ministry in MINISTRY_KEYWORDS.values())
    })

if __name__ == '__main__':
    print("üöÄ Starting Enhanced News Scraper Server v4.0...")
    print("üìä Available endpoints:")
    print("  ‚Ä¢ GET /api/news - Get all news")
    print("  ‚Ä¢ GET /api/news?language=bengali - Get Bengali news")
    print("  ‚Ä¢ GET /api/news?ministry=health - Get health ministry news")
    print("  ‚Ä¢ GET /api/news?language=hindi&ministry=finance - Get Hindi finance news")
    print("  ‚Ä¢ GET /api/languages - Get available languages")
    print("  ‚Ä¢ GET /api/ministries - Get available ministries")
    print("  ‚Ä¢ GET /api/sources - Get news sources")
    print("  ‚Ä¢ GET /api/health - Health check")
    print("üåê Server running on http://localhost:5000")
    print("üîß v4.0 Improvements:")
    print("  ‚úÖ Comprehensive multilingual keyword system")
    print("  ‚úÖ Weighted priority scoring for better accuracy")
    print("  ‚úÖ Confidence scoring for each categorization")
    print("  ‚úÖ Better content extraction with multiple strategies")
    print("  ‚úÖ Retry mechanism for failed requests")
    print("  ‚úÖ Enhanced duplicate detection")
    print("  ‚úÖ Added Agriculture ministry category")
    app.run(debug=True, port=5000)